{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccdb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 1. Charger les données ===\n",
    "data = np.load(\"training_data.npz\", allow_pickle=True)\n",
    "X = np.array(data[\"data\"], dtype=np.float32)  # conversion importante !\n",
    "feature_labels = data[\"feature_labels\"]\n",
    "\n",
    "# Charger les labels\n",
    "y = pd.read_csv(\"training_labels.csv\")\n",
    "y = y.iloc[:, 0].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8f7fc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of derivatives at boundaries does not match: expected 1, got 0+0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m                 X_filled[sample_idx, :, feature_idx] \u001b[38;5;241m=\u001b[39m f_interp(np\u001b[38;5;241m.\u001b[39marange(t))\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_filled\n\u001b[1;32m---> 52\u001b[0m X_filled \u001b[38;5;241m=\u001b[39m \u001b[43madaptive_interpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Step 2: Outlier removal (clip to mean ± 5 * std)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m X_no_outliers \u001b[38;5;241m=\u001b[39m X_filled\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36madaptive_interpolate\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     45\u001b[0m             X_filled[sample_idx, :, feature_idx] \u001b[38;5;241m=\u001b[39m f_interp(np\u001b[38;5;241m.\u001b[39marange(t))\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m             f_interp \u001b[38;5;241m=\u001b[39m \u001b[43minterp1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcubic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextrapolate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m             X_filled[sample_idx, :, feature_idx] \u001b[38;5;241m=\u001b[39m f_interp(np\u001b[38;5;241m.\u001b[39marange(t))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_filled\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\scipy\\interpolate\\_interpolate.py:397\u001b[0m, in \u001b[0;36minterp1d.__init__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    394\u001b[0m         yy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y)\n\u001b[0;32m    395\u001b[0m         rewrite_nan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spline \u001b[38;5;241m=\u001b[39m \u001b[43mmake_interp_spline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rewrite_nan:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_call_nan_spline\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\scipy\\interpolate\\_bsplines.py:1596\u001b[0m, in \u001b[0;36mmake_interp_spline\u001b[1;34m(x, y, k, t, bc_type, axis, check_finite)\u001b[0m\n\u001b[0;32m   1593\u001b[0m nt \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nt \u001b[38;5;241m-\u001b[39m n \u001b[38;5;241m!=\u001b[39m nleft \u001b[38;5;241m+\u001b[39m nright:\n\u001b[1;32m-> 1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of derivatives at boundaries does not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1597\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnt\u001b[38;5;241m-\u001b[39mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnleft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnright\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;66;03m# bail out if the `y` array is zero-sized\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The number of derivatives at boundaries does not match: expected 1, got 0+0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X is your raw input of shape (n_samples, 12, 77)\n",
    "'''\n",
    "# Step 1: Mean Imputation per feature\n",
    "X_imputed = X.copy()\n",
    "for f in range(X.shape[2]):\n",
    "    feature_vals = X[:, :, f]\n",
    "    feature_mean = np.nanmedian(feature_vals)\n",
    "    X_imputed[:, :, f] = np.nan_to_num(feature_vals, nan=feature_mean)\n",
    "'''\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def adaptive_interpolate(X):\n",
    "    \"\"\"\n",
    "    Interpolates missing values in a 3D array X of shape (n_samples, time_steps, n_features),\n",
    "    using an adaptive method depending on how many valid (non-NaN) points there are per sequence.\n",
    "\n",
    "    - 0 valid points → fill with global mean\n",
    "    - 1 point        → fill with that single value\n",
    "    - 2 points       → linear interpolation\n",
    "    - 3+ points      → cubic interpolation\n",
    "    \"\"\"\n",
    "    n, t, f = X.shape\n",
    "    X_filled = np.empty_like(X)\n",
    "    \n",
    "    # Fallback: global means for each feature\n",
    "    global_means = np.nanmean(X, axis=(0, 1))  # shape (f,)\n",
    "\n",
    "    for sample_idx in range(n):\n",
    "        for feature_idx in range(f):\n",
    "            ts = X[sample_idx, :, feature_idx]  # time series\n",
    "            valid_mask = ~np.isnan(ts)\n",
    "            valid_times = np.where(valid_mask)[0]\n",
    "            valid_values = ts[valid_mask]\n",
    "            \n",
    "            if len(valid_values) == 0:\n",
    "                X_filled[sample_idx, :, feature_idx] = global_means[feature_idx]\n",
    "            elif len(valid_values) == 1:\n",
    "                X_filled[sample_idx, :, feature_idx] = valid_values[0]\n",
    "            elif len(valid_values) == 2:\n",
    "                f_interp = interp1d(valid_times, valid_values, kind='linear', fill_value=\"extrapolate\")\n",
    "                X_filled[sample_idx, :, feature_idx] = f_interp(np.arange(t))\n",
    "            else:\n",
    "                f_interp = interp1d(valid_times, valid_values, kind='cubic', fill_value=\"extrapolate\")\n",
    "                X_filled[sample_idx, :, feature_idx] = f_interp(np.arange(t))\n",
    "    \n",
    "    return X_filled\n",
    "\n",
    "X_filled = adaptive_interpolate(X)\n",
    "\n",
    "# Step 2: Outlier removal (clip to mean ± 5 * std)\n",
    "X_no_outliers = X_filled.copy()\n",
    "for f in range(X.shape[2]):\n",
    "    mean = np.nanmean(X_no_outliers[:, :, f])\n",
    "    std = np.nanstd(X_no_outliers[:, :, f])\n",
    "    lower_bound = mean - 5 * std\n",
    "    upper_bound = mean + 5 * std\n",
    "    X_no_outliers[:, :, f] = np.clip(X_no_outliers[:, :, f], lower_bound, upper_bound)\n",
    "\n",
    "# Step 3: Standard Scaling (per feature, over all samples and time)\n",
    "n, t, f = X_no_outliers.shape\n",
    "X_flat = X_no_outliers.reshape(-1, f)  # shape (n * t, f)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "X_ready = X_scaled_flat.reshape(n, t, f)  # final shape: (n_samples, 12, 77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fade83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (37556, 12, 77)\n",
      "Val shape: (16096, 12, 77)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# y should be a 1D numpy array of 0/1 labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_ready, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a4f3b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.pos_indices = np.where(self.labels == 1)[0].tolist()\n",
    "        self.neg_indices = np.where(self.labels == 0)[0].tolist()\n",
    "        self.batch_half = batch_size // 2\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle both positive and negative indices each epoch\n",
    "        random.shuffle(self.pos_indices)\n",
    "        random.shuffle(self.neg_indices)\n",
    "\n",
    "        pos_iter = iter(self.pos_indices)\n",
    "        neg_iter = iter(self.neg_indices)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                pos_batch = [next(pos_iter) for _ in range(self.batch_half)]\n",
    "                neg_batch = [next(neg_iter) for _ in range(self.batch_half)]\n",
    "                batch = pos_batch + neg_batch\n",
    "                random.shuffle(batch)\n",
    "                yield batch\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.pos_indices), len(self.neg_indices)) // self.batch_half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eefb6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Assume X and y are NumPy arrays\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DiabetesDataset(X_train, y_train)\n",
    "train_sampler = BalancedBatchSampler(y_train, batch_size=64)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "val_dataset = DiabetesDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0c6d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=77, hidden_size=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        direction_multiplier = 2 if bidirectional else 1\n",
    "        self.norm = nn.LayerNorm(hidden_size * direction_multiplier)\n",
    "        self.fc = nn.Linear(hidden_size * direction_multiplier, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        normalized = self.norm(last_output)\n",
    "        logits = self.fc(normalized)\n",
    "        return logits.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2588bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7811a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Positive samples: 2375, Negative samples: 35181\n",
      "⚖️ Using pos_weight = 14.8131\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Count positives and negatives in training set\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "\n",
    "# Compute pos_weight: how much more to weigh the positive class\n",
    "pos_weight_value = n_neg / n_pos\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "\n",
    "print(f\"📊 Positive samples: {n_pos}, Negative samples: {n_neg}\")\n",
    "print(f\"⚖️ Using pos_weight = {pos_weight.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5089c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb233a9",
   "metadata": {},
   "source": [
    "# ***LSTM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59ee1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 30.4484 | Acc: 0.9364 | F1@best_thresh: 0.3970 | AUC: 0.8815\n",
      "Epoch 02 | Loss: 26.1085 | Acc: 0.9365 | F1@best_thresh: 0.4178 | AUC: 0.8912\n",
      "Epoch 03 | Loss: 24.9691 | Acc: 0.9399 | F1@best_thresh: 0.4434 | AUC: 0.8990\n",
      "Epoch 04 | Loss: 24.1282 | Acc: 0.9410 | F1@best_thresh: 0.4482 | AUC: 0.9047\n",
      "Epoch 05 | Loss: 23.6814 | Acc: 0.9406 | F1@best_thresh: 0.4471 | AUC: 0.9025\n",
      "Epoch 06 | Loss: 23.0627 | Acc: 0.9396 | F1@best_thresh: 0.4358 | AUC: 0.9009\n",
      "Epoch 07 | Loss: 22.5571 | Acc: 0.9402 | F1@best_thresh: 0.4457 | AUC: 0.8981\n",
      "Epoch 08 | Loss: 21.9607 | Acc: 0.9345 | F1@best_thresh: 0.4246 | AUC: 0.8907\n",
      "Epoch 09 | Loss: 20.6996 | Acc: 0.9400 | F1@best_thresh: 0.4621 | AUC: 0.8986\n",
      "Epoch 10 | Loss: 20.1351 | Acc: 0.9408 | F1@best_thresh: 0.4502 | AUC: 0.8942\n",
      "Epoch 11 | Loss: 19.7469 | Acc: 0.9409 | F1@best_thresh: 0.4594 | AUC: 0.8922\n",
      "Epoch 12 | Loss: 19.4916 | Acc: 0.9397 | F1@best_thresh: 0.4593 | AUC: 0.8864\n",
      "Epoch 13 | Loss: 19.0524 | Acc: 0.9393 | F1@best_thresh: 0.4589 | AUC: 0.8909\n",
      "Epoch 14 | Loss: 18.2890 | Acc: 0.9402 | F1@best_thresh: 0.4556 | AUC: 0.8892\n",
      "Epoch 15 | Loss: 17.9235 | Acc: 0.9394 | F1@best_thresh: 0.4572 | AUC: 0.8822\n",
      "Epoch 16 | Loss: 17.8078 | Acc: 0.9399 | F1@best_thresh: 0.4524 | AUC: 0.8850\n",
      "Epoch 17 | Loss: 17.4936 | Acc: 0.9395 | F1@best_thresh: 0.4491 | AUC: 0.8755\n",
      "Epoch 18 | Loss: 16.9132 | Acc: 0.9389 | F1@best_thresh: 0.4566 | AUC: 0.8799\n",
      "Epoch 19 | Loss: 16.8090 | Acc: 0.9379 | F1@best_thresh: 0.4495 | AUC: 0.8775\n",
      "Epoch 20 | Loss: 16.6960 | Acc: 0.9389 | F1@best_thresh: 0.4493 | AUC: 0.8753\n",
      "Epoch 21 | Loss: 16.4551 | Acc: 0.9390 | F1@best_thresh: 0.4579 | AUC: 0.8740\n",
      "Epoch 22 | Loss: 16.1812 | Acc: 0.9390 | F1@best_thresh: 0.4609 | AUC: 0.8745\n",
      "Epoch 23 | Loss: 16.0933 | Acc: 0.9386 | F1@best_thresh: 0.4502 | AUC: 0.8713\n",
      "Epoch 24 | Loss: 16.0344 | Acc: 0.9387 | F1@best_thresh: 0.4537 | AUC: 0.8734\n",
      "Epoch 25 | Loss: 15.8589 | Acc: 0.9383 | F1@best_thresh: 0.4482 | AUC: 0.8740\n",
      "Epoch 26 | Loss: 15.7889 | Acc: 0.9377 | F1@best_thresh: 0.4483 | AUC: 0.8717\n",
      "Epoch 27 | Loss: 15.7255 | Acc: 0.9377 | F1@best_thresh: 0.4489 | AUC: 0.8701\n",
      "Epoch 28 | Loss: 15.5992 | Acc: 0.9385 | F1@best_thresh: 0.4522 | AUC: 0.8679\n",
      "Epoch 29 | Loss: 15.5875 | Acc: 0.9384 | F1@best_thresh: 0.4498 | AUC: 0.8697\n",
      "Epoch 30 | Loss: 15.4600 | Acc: 0.9385 | F1@best_thresh: 0.4531 | AUC: 0.8689\n",
      "\n",
      "✅ Best F1-score: 0.4621 achieved at epoch 9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = ImprovedLSTM().to(device)\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "# Focal Loss instead of BCEWithLogitsLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader, y_train):\n",
    "    model = ImprovedLSTM().to(device)\n",
    "\n",
    "    # Compute pos_weight for imbalance\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        return np.array(y_true), np.array(y_prob)\n",
    "\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        y_true, y_prob = evaluate(model, val_loader)\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        f1_scores = [f1_score(y_true, y_prob > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_epoch_f1 = max(f1_scores)\n",
    "        acc = accuracy_score(y_true, y_prob > 0.5)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        if best_epoch_f1 > best_f1:\n",
    "            best_f1 = best_epoch_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model_lstm.pth\")\n",
    "\n",
    "        scheduler.step(best_epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1@best_thresh: {best_epoch_f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n✅ Best F1-score: {best_f1:.4f} achieved at epoch {best_epoch}\")\n",
    "    return model, best_thresh\n",
    "trained_model, threshold = train_model(train_loader, val_loader, y_train)\n",
    "# ➤ Call it like this:\n",
    "# trained_model, threshold = train_model(train_loader, val_loader, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a8912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff90b3c7",
   "metadata": {},
   "source": [
    "# ***CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b2c3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedCNNModel(nn.Module):\n",
    "    def __init__(self, input_channels=77, kernel_size=3, dropout=0.3):\n",
    "        super(ImprovedCNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, seq_len=12, features=77) → (batch, features, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x).squeeze(-1)  # Shape: (batch, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()  # Output: (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f6770976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 31.8442 | Acc: 0.9386 | F1@best_thresh: 0.3883 | AUC: 0.8825\n",
      "Epoch 02 | Loss: 27.5463 | Acc: 0.9392 | F1@best_thresh: 0.3990 | AUC: 0.8887\n",
      "Epoch 03 | Loss: 26.3977 | Acc: 0.9380 | F1@best_thresh: 0.4183 | AUC: 0.8928\n",
      "Epoch 04 | Loss: 25.5559 | Acc: 0.9394 | F1@best_thresh: 0.4133 | AUC: 0.8920\n",
      "Epoch 05 | Loss: 25.0849 | Acc: 0.9396 | F1@best_thresh: 0.4352 | AUC: 0.8963\n",
      "Epoch 06 | Loss: 24.8954 | Acc: 0.9391 | F1@best_thresh: 0.4123 | AUC: 0.8931\n",
      "Epoch 07 | Loss: 24.3378 | Acc: 0.9404 | F1@best_thresh: 0.4340 | AUC: 0.8992\n",
      "Epoch 08 | Loss: 24.2240 | Acc: 0.9409 | F1@best_thresh: 0.4223 | AUC: 0.8950\n",
      "Epoch 09 | Loss: 23.8462 | Acc: 0.9394 | F1@best_thresh: 0.4352 | AUC: 0.9005\n",
      "Epoch 10 | Loss: 22.7405 | Acc: 0.9407 | F1@best_thresh: 0.4343 | AUC: 0.9016\n",
      "Epoch 11 | Loss: 22.5443 | Acc: 0.9404 | F1@best_thresh: 0.4385 | AUC: 0.9001\n",
      "Epoch 12 | Loss: 22.2127 | Acc: 0.9400 | F1@best_thresh: 0.4354 | AUC: 0.8998\n",
      "Epoch 13 | Loss: 22.1730 | Acc: 0.9402 | F1@best_thresh: 0.4497 | AUC: 0.9039\n",
      "Epoch 14 | Loss: 21.6549 | Acc: 0.9404 | F1@best_thresh: 0.4432 | AUC: 0.8990\n",
      "Epoch 15 | Loss: 21.6058 | Acc: 0.9402 | F1@best_thresh: 0.4461 | AUC: 0.9000\n",
      "Epoch 16 | Loss: 21.3928 | Acc: 0.9400 | F1@best_thresh: 0.4387 | AUC: 0.8968\n",
      "Epoch 17 | Loss: 21.2661 | Acc: 0.9414 | F1@best_thresh: 0.4361 | AUC: 0.8948\n",
      "Epoch 18 | Loss: 20.6691 | Acc: 0.9407 | F1@best_thresh: 0.4493 | AUC: 0.8989\n",
      "Epoch 19 | Loss: 20.3321 | Acc: 0.9418 | F1@best_thresh: 0.4502 | AUC: 0.8966\n",
      "Epoch 20 | Loss: 20.2466 | Acc: 0.9407 | F1@best_thresh: 0.4426 | AUC: 0.8941\n",
      "Epoch 21 | Loss: 20.1753 | Acc: 0.9402 | F1@best_thresh: 0.4463 | AUC: 0.8957\n",
      "Epoch 22 | Loss: 19.9509 | Acc: 0.9412 | F1@best_thresh: 0.4503 | AUC: 0.8952\n",
      "Epoch 23 | Loss: 19.8619 | Acc: 0.9409 | F1@best_thresh: 0.4492 | AUC: 0.8942\n",
      "Epoch 24 | Loss: 19.6295 | Acc: 0.9401 | F1@best_thresh: 0.4527 | AUC: 0.8941\n",
      "Epoch 25 | Loss: 19.7763 | Acc: 0.9405 | F1@best_thresh: 0.4465 | AUC: 0.8947\n",
      "Epoch 26 | Loss: 19.5621 | Acc: 0.9407 | F1@best_thresh: 0.4372 | AUC: 0.8924\n",
      "Epoch 27 | Loss: 19.4517 | Acc: 0.9409 | F1@best_thresh: 0.4492 | AUC: 0.8947\n",
      "Epoch 28 | Loss: 19.2639 | Acc: 0.9409 | F1@best_thresh: 0.4515 | AUC: 0.8947\n",
      "Epoch 29 | Loss: 18.8892 | Acc: 0.9402 | F1@best_thresh: 0.4516 | AUC: 0.8934\n",
      "Epoch 30 | Loss: 18.7610 | Acc: 0.9408 | F1@best_thresh: 0.4527 | AUC: 0.8906\n",
      "\n",
      "✅ Best F1-score: 0.4527 achieved at epoch 24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "# Focal Loss instead of BCEWithLogitsLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure your model and FocalLoss are defined elsewhere\n",
    "# class DeepCNN(nn.Module): ...\n",
    "# class FocalLoss(nn.Module): ...\n",
    "\n",
    "def train_model(train_loader, val_loader, y_train):\n",
    "    model = ImprovedCNNModel().to(device)\n",
    "\n",
    "    # Compute pos_weight for imbalance\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        return np.array(y_true), np.array(y_prob)\n",
    "\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        y_true, y_prob = evaluate(model, val_loader)\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        f1_scores = [f1_score(y_true, y_prob > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_epoch_f1 = max(f1_scores)\n",
    "        acc = accuracy_score(y_true, y_prob > 0.5)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        if best_epoch_f1 > best_f1:\n",
    "            best_f1 = best_epoch_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model_cnn.pth\")\n",
    "\n",
    "        scheduler.step(best_epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1@best_thresh: {best_epoch_f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n✅ Best F1-score: {best_f1:.4f} achieved at epoch {best_epoch}\")\n",
    "    return model, best_thresh\n",
    "trained_model, threshold = train_model(train_loader, val_loader, y_train)\n",
    "# ➤ Call it like this:\n",
    "# trained_model, threshold = train_model(train_loader, val_loader, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a4c82d",
   "metadata": {},
   "source": [
    "# ***transformer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e7013c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_size=308, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=100)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        x = self.input_projection(x)  # (batch, seq_len, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]  # use the last time step's representation\n",
    "        x = self.norm(x)\n",
    "        return self.head(x).squeeze()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89757be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 32.1412 | Acc: 0.9368 | F1@best_thresh: 0.3737 | AUC: 0.8796\n",
      "Epoch 02 | Loss: 28.6734 | Acc: 0.9368 | F1@best_thresh: 0.4092 | AUC: 0.8833\n",
      "Epoch 03 | Loss: 30.1247 | Acc: 0.9368 | F1@best_thresh: 0.3297 | AUC: 0.7962\n",
      "Epoch 04 | Loss: 32.8621 | Acc: 0.9368 | F1@best_thresh: 0.2293 | AUC: 0.7947\n",
      "Epoch 05 | Loss: 34.2391 | Acc: 0.9368 | F1@best_thresh: 0.1412 | AUC: 0.4851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Best F1-score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m achieved at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, best_thresh\n\u001b[1;32m---> 72\u001b[0m trained_model, threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[132], line 46\u001b[0m, in \u001b[0;36mtrain_transformer_model\u001b[1;34m(train_loader, val_loader, y_train)\u001b[0m\n\u001b[0;32m     44\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[0;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[131], line 33\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)  \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[1;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# use the last time step's representation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:514\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    511\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 514\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    522\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:916\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    913\u001b[0m         x\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    915\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:941\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 941\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Focal Loss (assuming it's already defined) ─────────────────────────────\n",
    "# class FocalLoss(nn.Module): ...\n",
    "\n",
    "# ─── Transformer model (you already defined it) ─────────────────────────────\n",
    "# class TimeSeriesTransformer(nn.Module): ...\n",
    "\n",
    "def train_transformer_model(train_loader, val_loader, y_train):\n",
    "    model = TimeSeriesTransformer(input_size=77).to(device)  # ← change input_size if needed\n",
    "\n",
    "    # Class imbalance handling\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        return np.array(y_true), np.array(y_prob)\n",
    "\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        y_true, y_prob = evaluate(model, val_loader)\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        f1_scores = [f1_score(y_true, y_prob > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_epoch_f1 = max(f1_scores)\n",
    "        acc = accuracy_score(y_true, y_prob > 0.5)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        if best_epoch_f1 > best_f1:\n",
    "            best_f1 = best_epoch_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model_transformer.pth\")\n",
    "\n",
    "        scheduler.step(best_epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1@best_thresh: {best_epoch_f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n✅ Best F1-score: {best_f1:.4f} achieved at epoch {best_epoch}\")\n",
    "    return model, best_thresh\n",
    "\n",
    "trained_model, threshold = train_transformer_model(train_loader, val_loader, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51757369",
   "metadata": {},
   "source": [
    "# ***fusion***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feda00a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedCNNModel(\n",
       "  (conv1): Conv1d(77, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = ImprovedLSTM().to(device)\n",
    "cnn_model = ImprovedCNNModel().to(device)\n",
    "\n",
    "lstm_model.load_state_dict(torch.load(\"best_model_lstm.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"best_model_cnn.pth\"))\n",
    "\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "687a5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_probs(model, dataloader):\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "    return np.array(probs)\n",
    "\n",
    "y_true = []\n",
    "for _, y_batch in val_loader:\n",
    "    y_true.extend(y_batch.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "probs_lstm = get_model_probs(lstm_model, val_loader)\n",
    "probs_cnn  = get_model_probs(cnn_model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82171006",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_fused = 0.6 * probs_lstm + 0.4 * probs_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b833bafe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10731, 26826]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m81\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m f1s \u001b[38;5;241m=\u001b[39m [f1_score(y_true, probs_fused \u001b[38;5;241m>\u001b[39m t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m thresholds]\n\u001b[0;32m      3\u001b[0m best_thresh \u001b[38;5;241m=\u001b[39m thresholds[np\u001b[38;5;241m.\u001b[39margmax(f1s)]\n\u001b[0;32m      5\u001b[0m final_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(f1s)\n",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m81\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m f1s \u001b[38;5;241m=\u001b[39m [\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs_fused\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m thresholds]\n\u001b[0;32m      3\u001b[0m best_thresh \u001b[38;5;241m=\u001b[39m thresholds[np\u001b[38;5;241m.\u001b[39margmax(f1s)]\n\u001b[0;32m      5\u001b[0m final_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(f1s)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1324\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1145\u001b[0m     {\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1172\u001b[0m ):\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1337\u001b[0m     {\n\u001b[0;32m   1338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1366\u001b[0m ):\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \n\u001b[0;32m   1369\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;124;03m    0.12...\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1517\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1830\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1830\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1596\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m   1595\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m-> 1596\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m _tolist(unique_labels(y_true, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Challenge-data-mias\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10731, 26826]"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "f1s = [f1_score(y_true, probs_fused > t) for t in thresholds]\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "\n",
    "final_f1 = max(f1s)\n",
    "final_acc = accuracy_score(y_true, probs_fused > 0.5)\n",
    "final_auc = roc_auc_score(y_true, probs_fused)\n",
    "\n",
    "print(f\"\\n📊 Fusion Results:\")\n",
    "print(f\"AUC: {final_auc:.4f} | Acc: {final_acc:.4f} | Best F1: {final_f1:.4f} @ Threshold: {best_thresh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18453e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (probs_fused > best_thresh).astype(int)\n",
    "errors = (y_pred != y_true).astype(int)  # 1 if misclassified, 0 if correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230215dc",
   "metadata": {},
   "source": [
    "# ***meta learner***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a1daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Fusion Results (Meta + Biological Features):\n",
      "AUC: 0.9062 | Acc: 0.9419 | Best F1: 0.4829 @ Threshold: 0.23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# ─── Load models ────────────────────────────────────────────────────────────\n",
    "lstm_model = ImprovedLSTM().to(device)\n",
    "cnn_model = ImprovedCNNModel().to(device)\n",
    "\n",
    "lstm_model.load_state_dict(torch.load(\"best_model_lstm.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"best_model_cnn.pth\"))\n",
    "\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n",
    "\n",
    "# ─── Get model probabilities ────────────────────────────────────────────────\n",
    "def get_model_probs(model, dataloader):\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "    return np.array(probs)\n",
    "\n",
    "# ─── True labels ────────────────────────────────────────────────────────────\n",
    "y_true = []\n",
    "for _, y_batch in val_loader:\n",
    "    y_true.extend(y_batch.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# ─── Probabilities from each model ──────────────────────────────────────────\n",
    "probs_lstm = get_model_probs(lstm_model, val_loader)\n",
    "probs_cnn  = get_model_probs(cnn_model, val_loader)\n",
    "\n",
    "# ─── Train meta-learner on model predictions ────────────────────────────────\n",
    "X_stack = np.vstack([probs_lstm, probs_cnn]).T  # shape: (n_samples, 2)\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X_stack, y_true)\n",
    "\n",
    "# ─── Fused probability from meta-learner ────────────────────────────────────\n",
    "probs_fused = meta_model.predict_proba(X_stack)[:, 1]\n",
    "\n",
    "# ─── Find best F1 threshold ─────────────────────────────────────────────────\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "f1s = [f1_score(y_true, probs_fused > t) for t in thresholds]\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "\n",
    "# ─── Final Metrics ──────────────────────────────────────────────────────────\n",
    "final_f1 = max(f1s)\n",
    "final_acc = accuracy_score(y_true, probs_fused > 0.5)\n",
    "final_auc = roc_auc_score(y_true, probs_fused)\n",
    "\n",
    "print(f\"\\n📊 Fusion Results (Meta-Learner):\")\n",
    "print(f\"AUC: {final_auc:.4f} | Acc: {final_acc:.4f} | Best F1: {final_f1:.4f} @ Threshold: {best_thresh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "62130fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta_model.pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(meta_model, \"meta_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7538df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (probs_fused > best_thresh).astype(int)\n",
    "errors = (y_pred != y_true).astype(int)  # 1 if misclassified, 0 if correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce608126",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_X = np.mean(X_val, axis=1)  # shape: (n_patients, n_features)\n",
    "\n",
    "meta_y = errors  # from step above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4806591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      9811\n",
      "           1       0.96      0.97      0.97       920\n",
      "\n",
      "    accuracy                           0.99     10731\n",
      "   macro avg       0.98      0.98      0.98     10731\n",
      "weighted avg       0.99      0.99      0.99     10731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "meta_model = RandomForestClassifier()\n",
    "meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "print(classification_report(meta_y, meta_model.predict(meta_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "90cfb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = meta_model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53d74bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_idx = np.where(errors == 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5338eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_patients = X_val[misclassified_idx]        # shape: (n_hard, t, f)\n",
    "hard_labels   = y_true[misclassified_idx]       # true labels\n",
    "hard_preds    = y_pred[misclassified_idx]       # wrong predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb422be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hard patients: 920\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total hard patients: {len(hard_patients)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83bb2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top features that differ between hard and easy patients:\n",
      "\n",
      "F41    0.169694\n",
      "F42    0.129216\n",
      "F30    0.117308\n",
      "F51    0.104652\n",
      "F5     0.098290\n",
      "F6     0.090720\n",
      "F55    0.089622\n",
      "F3     0.087442\n",
      "F63    0.086043\n",
      "F66    0.085787\n",
      "F65    0.085770\n",
      "F59    0.085769\n",
      "F52    0.085765\n",
      "F61    0.085722\n",
      "F58    0.085450\n",
      "F60    0.085314\n",
      "F50    0.085014\n",
      "F56    0.084907\n",
      "F68    0.083082\n",
      "F67    0.082888\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mean feature values over time\n",
    "hard_means = np.nanmean(hard_patients, axis=1)  # shape: (n_hard, f)\n",
    "easy_means = np.nanmean(X_val[errors == 0], axis=1)\n",
    "\n",
    "df_hard = pd.DataFrame(hard_means, columns=[f'F{i}' for i in range(hard_means.shape[1])])\n",
    "df_easy = pd.DataFrame(easy_means, columns=[f'F{i}' for i in range(easy_means.shape[1])])\n",
    "\n",
    "# Compare feature-wise averages\n",
    "diff = df_hard.mean() - df_easy.mean()\n",
    "top_diff = diff.abs().sort_values(ascending=False).head(20)\n",
    "print(\"🔍 Top features that differ between hard and easy patients:\\n\")\n",
    "print(top_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08e68933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F41: inr\n",
      "F42: inr_a\n",
      "F30: hba1c\n",
      "F51: platelets\n",
      "F5: ast\n",
      "F6: alt\n",
      "F55: lymphocytes\n",
      "F3: prothrombin_time\n",
      "F63: hyperbasophilic_lymphocytes\n",
      "F66: atypical_lymphocytes\n",
      "F65: villous_lymphocytes\n",
      "F59: granular_lymphocytes\n",
      "F52: plasma_cells\n",
      "F61: hemoglobin\n",
      "F58: myelocytes\n",
      "F60: erythroblasts\n",
      "F50: hematocrit\n",
      "F56: mchc\n",
      "F68: mcv\n",
      "F67: mch\n"
     ]
    }
   ],
   "source": [
    "top_feat_ids = top_diff.index.str.extract(r'F(\\d+)').astype(int)[0]\n",
    "\n",
    "for i in top_feat_ids:\n",
    "    print(f\"F{i}: {feature_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aad0acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_id = 63 # Example, replace with top ones like 41, 42, etc.\n",
    "hard_mean = np.nanmean(hard_patients[:, :, feature_id], axis=0)\n",
    "easy_mean = np.nanmean(X_val[errors == 0][:, :, feature_id], axis=0)\n",
    "\n",
    "plt.plot(hard_mean, label=\"Hard patients\")\n",
    "plt.plot(easy_mean, label=\"Easy patients\")\n",
    "plt.title(f\"Temporal Pattern - Feature: {feature_labels[feature_id]}\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20bd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37777737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c861d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9178678b",
   "metadata": {},
   "source": [
    "# ***testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0bc97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file saved as final_submission.csv with binary labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Load test data\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "data = np.load(\"evaluation_data.npz\", allow_pickle=True)\n",
    "X_test = np.array(data[\"data\"], dtype=np.float32)  # conversion importante !\n",
    "feature_labels = data[\"feature_labels\"]\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Apply preprocessing (same as training pipeline)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "X_imputed = X_test.copy()\n",
    "for f in range(X_test.shape[2]):\n",
    "    feature_vals = X_test[:, :, f]\n",
    "    X_imputed[:, :, f] = np.nan_to_num(feature_vals, nan=0.0)\n",
    "\n",
    "# Step 2: Outlier clipping (mean ± 5*std)\n",
    "for f in range(X_test.shape[2]):\n",
    "    mean = np.nanmean(X_test[:, :, f])\n",
    "    std = np.nanstd(X_test[:, :, f])\n",
    "    lower = mean - 5 * std\n",
    "    upper = mean + 5 * std\n",
    "    X_test[:, :, f] = np.clip(X_test[:, :, f], lower, upper)\n",
    "\n",
    "# Step 3: Standard Scaling\n",
    "n, t, f = X_test.shape\n",
    "X_flat = X_test.reshape(-1, f)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "X_test_ready = X_scaled_flat.reshape(n, t, f)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Convert to PyTorch dataloader (no labels here)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_tensor = torch.tensor(X_test_ready, dtype=torch.float32)\n",
    "dummy_labels = torch.zeros(len(X_tensor))  # dummy placeholder\n",
    "test_loader = DataLoader(TensorDataset(X_tensor, dummy_labels), batch_size=64, shuffle=False)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Load both trained models\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lstm_model = ImprovedLSTM().to(device)\n",
    "cnn_model = ImprovedCNNModel().to(device)\n",
    "lstm_model.load_state_dict(torch.load(\"best_model_lstm.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"best_model_cnn.pth\"))\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Predict probabilities and apply weighted fusion\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def get_model_probs(model, dataloader):\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "    return np.array(probs)\n",
    "\n",
    "probs_lstm = get_model_probs(lstm_model, test_loader)\n",
    "probs_cnn  = get_model_probs(cnn_model, test_loader)\n",
    "probs_fused = 0.4 * probs_lstm + 0.6 * probs_cnn\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Convert probs to class labels using threshold 0.39\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "best_thresh = 0.36\n",
    "labels = (probs_fused > best_thresh).astype(int).flatten()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Generate final submission file\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": np.arange(len(labels)),\n",
    "    \"Label\": labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"✅ Submission file saved as final_submission.csv with binary labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e2c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ccdb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 1. Charger les donn√©es ===\n",
    "data = np.load(\"training_data.npz\", allow_pickle=True)\n",
    "X = np.array(data[\"data\"], dtype=np.float32)  # conversion importante !\n",
    "feature_labels = data[\"feature_labels\"]\n",
    "\n",
    "# Charger les labels\n",
    "y = pd.read_csv(\"training_labels.csv\")\n",
    "y = y.iloc[:, 0].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a8f7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X is your raw input of shape (n_samples, 12, 77)\n",
    "\n",
    "# Step 1: Mean Imputation per feature\n",
    "X_imputed = X.copy()\n",
    "for f in range(X.shape[2]):\n",
    "    feature_vals = X[:, :, f]\n",
    "    feature_mean = np.nanmean(feature_vals)\n",
    "    X_imputed[:, :, f] = np.nan_to_num(feature_vals, nan=feature_mean)\n",
    "\n",
    "# Step 2: Outlier removal (clip to mean ¬± 5 * std)\n",
    "X_no_outliers = X_imputed.copy()\n",
    "for f in range(X.shape[2]):\n",
    "    mean = np.nanmean(X_no_outliers[:, :, f])\n",
    "    std = np.nanstd(X_no_outliers[:, :, f])\n",
    "    lower_bound = mean - 5 * std\n",
    "    upper_bound = mean + 5 * std\n",
    "    X_no_outliers[:, :, f] = np.clip(X_no_outliers[:, :, f], lower_bound, upper_bound)\n",
    "\n",
    "# Step 3: Standard Scaling (per feature, over all samples and time)\n",
    "n, t, f = X_no_outliers.shape\n",
    "X_flat = X_no_outliers.reshape(-1, f)  # shape (n * t, f)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "X_ready = X_scaled_flat.reshape(n, t, f)  # final shape: (n_samples, 12, 77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fade83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (42921, 12, 77)\n",
      "Val shape: (10731, 12, 77)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# y should be a 1D numpy array of 0/1 labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_ready, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4f3b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.pos_indices = np.where(self.labels == 1)[0].tolist()\n",
    "        self.neg_indices = np.where(self.labels == 0)[0].tolist()\n",
    "        self.batch_half = batch_size // 2\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle both positive and negative indices each epoch\n",
    "        random.shuffle(self.pos_indices)\n",
    "        random.shuffle(self.neg_indices)\n",
    "\n",
    "        pos_iter = iter(self.pos_indices)\n",
    "        neg_iter = iter(self.neg_indices)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                pos_batch = [next(pos_iter) for _ in range(self.batch_half)]\n",
    "                neg_batch = [next(neg_iter) for _ in range(self.batch_half)]\n",
    "                batch = pos_batch + neg_batch\n",
    "                random.shuffle(batch)\n",
    "                yield batch\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.pos_indices), len(self.neg_indices)) // self.batch_half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eefb6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Assume X and y are NumPy arrays\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DiabetesDataset(X_train, y_train)\n",
    "train_sampler = BalancedBatchSampler(y_train, batch_size=64)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "val_dataset = DiabetesDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0c6d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=77, hidden_size=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        direction_multiplier = 2 if bidirectional else 1\n",
    "        self.norm = nn.LayerNorm(hidden_size * direction_multiplier)\n",
    "        self.fc = nn.Linear(hidden_size * direction_multiplier, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        normalized = self.norm(last_output)\n",
    "        logits = self.fc(normalized)\n",
    "        return logits.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2588bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_term * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7811a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Positive samples: 2714, Negative samples: 40207\n",
      "‚öñÔ∏è Using pos_weight = 14.8147\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Count positives and negatives in training set\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "\n",
    "# Compute pos_weight: how much more to weigh the positive class\n",
    "pos_weight_value = n_neg / n_pos\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "\n",
    "print(f\"üìä Positive samples: {n_pos}, Negative samples: {n_neg}\")\n",
    "print(f\"‚öñÔ∏è Using pos_weight = {pos_weight.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5089c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb233a9",
   "metadata": {},
   "source": [
    "# ***LSTM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59ee1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 34.9065 | Acc: 0.9380 | F1@best_thresh: 0.4076 | AUC: 0.8951\n",
      "Epoch 02 | Loss: 29.0027 | Acc: 0.9369 | F1@best_thresh: 0.4519 | AUC: 0.9048\n",
      "Epoch 03 | Loss: 27.2355 | Acc: 0.9396 | F1@best_thresh: 0.4409 | AUC: 0.9056\n",
      "Epoch 04 | Loss: 26.2248 | Acc: 0.9399 | F1@best_thresh: 0.4468 | AUC: 0.9086\n",
      "Epoch 05 | Loss: 24.8561 | Acc: 0.9412 | F1@best_thresh: 0.4340 | AUC: 0.8994\n",
      "Epoch 06 | Loss: 23.9555 | Acc: 0.9373 | F1@best_thresh: 0.4467 | AUC: 0.9076\n",
      "Epoch 07 | Loss: 22.1093 | Acc: 0.9405 | F1@best_thresh: 0.4544 | AUC: 0.9027\n",
      "Epoch 08 | Loss: 21.1600 | Acc: 0.9409 | F1@best_thresh: 0.4548 | AUC: 0.9016\n",
      "Epoch 09 | Loss: 20.6512 | Acc: 0.9397 | F1@best_thresh: 0.4492 | AUC: 0.8982\n",
      "Epoch 10 | Loss: 20.0116 | Acc: 0.9414 | F1@best_thresh: 0.4467 | AUC: 0.8899\n",
      "Epoch 11 | Loss: 19.5735 | Acc: 0.9391 | F1@best_thresh: 0.4482 | AUC: 0.8908\n",
      "Epoch 12 | Loss: 18.9067 | Acc: 0.9405 | F1@best_thresh: 0.4387 | AUC: 0.8846\n",
      "Epoch 13 | Loss: 17.7164 | Acc: 0.9391 | F1@best_thresh: 0.4464 | AUC: 0.8822\n",
      "Epoch 14 | Loss: 17.0490 | Acc: 0.9366 | F1@best_thresh: 0.4399 | AUC: 0.8777\n",
      "Epoch 15 | Loss: 16.8686 | Acc: 0.9380 | F1@best_thresh: 0.4433 | AUC: 0.8729\n",
      "Epoch 16 | Loss: 16.5108 | Acc: 0.9364 | F1@best_thresh: 0.4364 | AUC: 0.8658\n",
      "Epoch 17 | Loss: 16.0028 | Acc: 0.9382 | F1@best_thresh: 0.4343 | AUC: 0.8684\n",
      "Epoch 18 | Loss: 15.6701 | Acc: 0.9378 | F1@best_thresh: 0.4378 | AUC: 0.8648\n",
      "Epoch 19 | Loss: 15.4705 | Acc: 0.9367 | F1@best_thresh: 0.4317 | AUC: 0.8632\n",
      "Epoch 20 | Loss: 15.2592 | Acc: 0.9363 | F1@best_thresh: 0.4298 | AUC: 0.8599\n",
      "Epoch 21 | Loss: 14.7873 | Acc: 0.9341 | F1@best_thresh: 0.4321 | AUC: 0.8587\n",
      "Epoch 22 | Loss: 14.6444 | Acc: 0.9363 | F1@best_thresh: 0.4279 | AUC: 0.8521\n",
      "Epoch 23 | Loss: 14.6257 | Acc: 0.9352 | F1@best_thresh: 0.4267 | AUC: 0.8520\n",
      "Epoch 24 | Loss: 14.4177 | Acc: 0.9354 | F1@best_thresh: 0.4284 | AUC: 0.8526\n",
      "Epoch 25 | Loss: 14.1338 | Acc: 0.9367 | F1@best_thresh: 0.4235 | AUC: 0.8477\n",
      "Epoch 26 | Loss: 14.0911 | Acc: 0.9352 | F1@best_thresh: 0.4209 | AUC: 0.8487\n",
      "Epoch 27 | Loss: 14.0587 | Acc: 0.9351 | F1@best_thresh: 0.4178 | AUC: 0.8476\n",
      "Epoch 28 | Loss: 13.8172 | Acc: 0.9357 | F1@best_thresh: 0.4226 | AUC: 0.8453\n",
      "Epoch 29 | Loss: 13.8269 | Acc: 0.9353 | F1@best_thresh: 0.4194 | AUC: 0.8453\n",
      "Epoch 30 | Loss: 13.7877 | Acc: 0.9356 | F1@best_thresh: 0.4194 | AUC: 0.8451\n",
      "\n",
      "‚úÖ Best F1-score: 0.4548 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = ImprovedLSTM().to(device)\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "# Focal Loss instead of BCEWithLogitsLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader, y_train):\n",
    "    model = ImprovedLSTM().to(device)\n",
    "\n",
    "    # Compute pos_weight for imbalance\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        return np.array(y_true), np.array(y_prob)\n",
    "\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        y_true, y_prob = evaluate(model, val_loader)\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        f1_scores = [f1_score(y_true, y_prob > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_epoch_f1 = max(f1_scores)\n",
    "        acc = accuracy_score(y_true, y_prob > 0.5)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        if best_epoch_f1 > best_f1:\n",
    "            best_f1 = best_epoch_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model_lstm.pth\")\n",
    "\n",
    "        scheduler.step(best_epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1@best_thresh: {best_epoch_f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Best F1-score: {best_f1:.4f} achieved at epoch {best_epoch}\")\n",
    "    return model, best_thresh\n",
    "trained_model, threshold = train_model(train_loader, val_loader, y_train)\n",
    "# ‚û§ Call it like this:\n",
    "# trained_model, threshold = train_model(train_loader, val_loader, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a8912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff90b3c7",
   "metadata": {},
   "source": [
    "# ***CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b2c3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedCNNModel(nn.Module):\n",
    "    def __init__(self, input_channels=77, kernel_size=3, dropout=0.3):\n",
    "        super(ImprovedCNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, seq_len=12, features=77) ‚Üí (batch, features, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x).squeeze(-1)  # Shape: (batch, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()  # Output: (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6770976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 37.0272 | Acc: 0.9372 | F1@best_thresh: 0.3976 | AUC: 0.8915\n",
      "Epoch 02 | Loss: 30.9534 | Acc: 0.9390 | F1@best_thresh: 0.4252 | AUC: 0.9010\n",
      "Epoch 03 | Loss: 29.3927 | Acc: 0.9416 | F1@best_thresh: 0.4272 | AUC: 0.9017\n",
      "Epoch 04 | Loss: 28.4255 | Acc: 0.9408 | F1@best_thresh: 0.4428 | AUC: 0.9071\n",
      "Epoch 05 | Loss: 27.6088 | Acc: 0.9420 | F1@best_thresh: 0.4391 | AUC: 0.9044\n",
      "Epoch 06 | Loss: 26.9163 | Acc: 0.9405 | F1@best_thresh: 0.4493 | AUC: 0.9053\n",
      "Epoch 07 | Loss: 26.3853 | Acc: 0.9405 | F1@best_thresh: 0.4458 | AUC: 0.9059\n",
      "Epoch 08 | Loss: 26.1959 | Acc: 0.9415 | F1@best_thresh: 0.4643 | AUC: 0.9093\n",
      "Epoch 09 | Loss: 25.4831 | Acc: 0.9425 | F1@best_thresh: 0.4441 | AUC: 0.9029\n",
      "Epoch 10 | Loss: 25.1691 | Acc: 0.9416 | F1@best_thresh: 0.4613 | AUC: 0.9087\n",
      "Epoch 11 | Loss: 24.7205 | Acc: 0.9412 | F1@best_thresh: 0.4588 | AUC: 0.9088\n",
      "Epoch 12 | Loss: 24.2569 | Acc: 0.9407 | F1@best_thresh: 0.4571 | AUC: 0.9009\n",
      "Epoch 13 | Loss: 23.1989 | Acc: 0.9402 | F1@best_thresh: 0.4535 | AUC: 0.9070\n",
      "Epoch 14 | Loss: 22.4005 | Acc: 0.9404 | F1@best_thresh: 0.4522 | AUC: 0.9053\n",
      "Epoch 15 | Loss: 22.2233 | Acc: 0.9380 | F1@best_thresh: 0.4486 | AUC: 0.9014\n",
      "Epoch 16 | Loss: 21.8961 | Acc: 0.9401 | F1@best_thresh: 0.4463 | AUC: 0.9001\n",
      "Epoch 17 | Loss: 21.0782 | Acc: 0.9405 | F1@best_thresh: 0.4556 | AUC: 0.8999\n",
      "Epoch 18 | Loss: 20.8693 | Acc: 0.9394 | F1@best_thresh: 0.4487 | AUC: 0.8983\n",
      "Epoch 19 | Loss: 20.6400 | Acc: 0.9399 | F1@best_thresh: 0.4446 | AUC: 0.8965\n",
      "Epoch 20 | Loss: 20.3921 | Acc: 0.9408 | F1@best_thresh: 0.4542 | AUC: 0.8965\n",
      "Epoch 21 | Loss: 20.0663 | Acc: 0.9414 | F1@best_thresh: 0.4485 | AUC: 0.8937\n",
      "Epoch 22 | Loss: 19.8475 | Acc: 0.9397 | F1@best_thresh: 0.4421 | AUC: 0.8956\n",
      "Epoch 23 | Loss: 19.7363 | Acc: 0.9403 | F1@best_thresh: 0.4478 | AUC: 0.8955\n",
      "Epoch 24 | Loss: 19.8130 | Acc: 0.9414 | F1@best_thresh: 0.4470 | AUC: 0.8934\n",
      "Epoch 25 | Loss: 19.5080 | Acc: 0.9395 | F1@best_thresh: 0.4355 | AUC: 0.8926\n",
      "Epoch 26 | Loss: 19.4596 | Acc: 0.9403 | F1@best_thresh: 0.4433 | AUC: 0.8918\n",
      "Epoch 27 | Loss: 19.3797 | Acc: 0.9398 | F1@best_thresh: 0.4429 | AUC: 0.8920\n",
      "Epoch 28 | Loss: 19.2142 | Acc: 0.9397 | F1@best_thresh: 0.4449 | AUC: 0.8916\n",
      "Epoch 29 | Loss: 19.1229 | Acc: 0.9406 | F1@best_thresh: 0.4404 | AUC: 0.8898\n",
      "Epoch 30 | Loss: 19.1986 | Acc: 0.9402 | F1@best_thresh: 0.4398 | AUC: 0.8849\n",
      "\n",
      "‚úÖ Best F1-score: 0.4643 achieved at epoch 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_pos = (y_train == 1).sum()\n",
    "n_neg = (y_train == 0).sum()\n",
    "pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "# Focal Loss instead of BCEWithLogitsLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure your model and FocalLoss are defined elsewhere\n",
    "# class DeepCNN(nn.Module): ...\n",
    "# class FocalLoss(nn.Module): ...\n",
    "\n",
    "def train_model(train_loader, val_loader, y_train):\n",
    "    model = ImprovedCNNModel().to(device)\n",
    "\n",
    "    # Compute pos_weight for imbalance\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor([n_neg / n_pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        return np.array(y_true), np.array(y_prob)\n",
    "\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for epoch in range(1, 31):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        y_true, y_prob = evaluate(model, val_loader)\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        f1_scores = [f1_score(y_true, y_prob > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "        best_epoch_f1 = max(f1_scores)\n",
    "        acc = accuracy_score(y_true, y_prob > 0.5)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "        if best_epoch_f1 > best_f1:\n",
    "            best_f1 = best_epoch_f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model_cnn.pth\")\n",
    "\n",
    "        scheduler.step(best_epoch_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1@best_thresh: {best_epoch_f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Best F1-score: {best_f1:.4f} achieved at epoch {best_epoch}\")\n",
    "    return model, best_thresh\n",
    "trained_model, threshold = train_model(train_loader, val_loader, y_train)\n",
    "# ‚û§ Call it like this:\n",
    "# trained_model, threshold = train_model(train_loader, val_loader, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51757369",
   "metadata": {},
   "source": [
    "# ***fusion***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feda00a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedCNNModel(\n",
       "  (conv1): Conv1d(77, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = ImprovedLSTM().to(device)\n",
    "cnn_model = ImprovedCNNModel().to(device)\n",
    "\n",
    "lstm_model.load_state_dict(torch.load(\"best_model_lstm.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"best_model_cnn.pth\"))\n",
    "\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "687a5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_probs(model, dataloader):\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "    return np.array(probs)\n",
    "\n",
    "y_true = []\n",
    "for _, y_batch in val_loader:\n",
    "    y_true.extend(y_batch.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "probs_lstm = get_model_probs(lstm_model, val_loader)\n",
    "probs_cnn  = get_model_probs(cnn_model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82171006",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_fused = 0.4 * probs_lstm + 0.6 * probs_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b833bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Fusion Results:\n",
      "AUC: 0.9138 | Acc: 0.9427 | Best F1: 0.4638 @ Threshold: 0.38\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "f1s = [f1_score(y_true, probs_fused > t) for t in thresholds]\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "\n",
    "final_f1 = max(f1s)\n",
    "final_acc = accuracy_score(y_true, probs_fused > 0.5)\n",
    "final_auc = roc_auc_score(y_true, probs_fused)\n",
    "\n",
    "print(f\"\\nüìä Fusion Results:\")\n",
    "print(f\"AUC: {final_auc:.4f} | Acc: {final_acc:.4f} | Best F1: {final_f1:.4f} @ Threshold: {best_thresh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC: 0.9138 | Acc: 0.9427 | Best F1: 0.4638 @ Threshold: 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178678b",
   "metadata": {},
   "source": [
    "# ***testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc0bc97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission file saved as final_submission.csv with binary labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Load test data\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "data = np.load(\"evaluation_data.npz\", allow_pickle=True)\n",
    "X_test = np.array(data[\"data\"], dtype=np.float32)  # conversion importante !\n",
    "feature_labels = data[\"feature_labels\"]\n",
    "\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Apply preprocessing (same as training pipeline)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# Step 1: Mean Imputation\n",
    "for f in range(X_test.shape[2]):\n",
    "    feature_vals = X_test[:, :, f]\n",
    "    feature_mean = np.nanmean(feature_vals)\n",
    "    X_test[:, :, f] = np.nan_to_num(feature_vals, nan=feature_mean)\n",
    "\n",
    "# Step 2: Outlier clipping (mean ¬± 5*std)\n",
    "for f in range(X_test.shape[2]):\n",
    "    mean = np.nanmean(X_test[:, :, f])\n",
    "    std = np.nanstd(X_test[:, :, f])\n",
    "    lower = mean - 5 * std\n",
    "    upper = mean + 5 * std\n",
    "    X_test[:, :, f] = np.clip(X_test[:, :, f], lower, upper)\n",
    "\n",
    "# Step 3: Standard Scaling\n",
    "n, t, f = X_test.shape\n",
    "X_flat = X_test.reshape(-1, f)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "X_test_ready = X_scaled_flat.reshape(n, t, f)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Convert to PyTorch dataloader (no labels here)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_tensor = torch.tensor(X_test_ready, dtype=torch.float32)\n",
    "dummy_labels = torch.zeros(len(X_tensor))  # dummy placeholder\n",
    "test_loader = DataLoader(TensorDataset(X_tensor, dummy_labels), batch_size=64, shuffle=False)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Load both trained models\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lstm_model = ImprovedLSTM().to(device)\n",
    "cnn_model = ImprovedCNNModel().to(device)\n",
    "lstm_model.load_state_dict(torch.load(\"best_model_lstm.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"best_model_cnn.pth\"))\n",
    "lstm_model.eval()\n",
    "cnn_model.eval()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Predict probabilities and apply weighted fusion\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_model_probs(model, dataloader):\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "    return np.array(probs)\n",
    "\n",
    "probs_lstm = get_model_probs(lstm_model, test_loader)\n",
    "probs_cnn  = get_model_probs(cnn_model, test_loader)\n",
    "probs_fused = 0.4 * probs_lstm + 0.6 * probs_cnn\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Convert probs to class labels using threshold 0.39\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "best_thresh = 0.39\n",
    "labels = (probs_fused > best_thresh).astype(int).flatten()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Generate final submission file\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": np.arange(len(labels)),\n",
    "    \"Label\": labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"‚úÖ Submission file saved as final_submission.csv with binary labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d94e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
